# Bias-Variance Tradeoff

## Introdu√ß√£o

O principal objetivo de um modelo de aprendizado de m√°quina (*machine learning*) supervisionado √© de aprender com um conjunto de dados e r√≥tulos (tamb√©m conhecidos como *features* e *target*) como generalizar caracter√≠sticas dessas informa√ß√µes em dados ainda n√£o vistos.

E acompanhando esse objetivo principal, vem o de medir de maneira objetiva o qu√£o bem esse nosso modelo de aprendizado generalizou essas caracter√≠sticas nos dados que foram apresentados pra ele. Dito isso, analisamos caracter√≠sticas como: qualidade do ajuste, erro da predi√ß√£o e vi√©s. Essa se faz uma etapa necess√°ria para entender o qu√£o √∫til esse modelo ser√° em nos ajudar nas duas tarefas principais que um modelo pode ter: *predi√ß√£o* ou *infer√™ncia*.

> ‚ÄúTodos os modelos est√£o errados, mas alguns s√£o √∫teis‚Äù - *George Box.*

Chamamos a capacidade de generaliza√ß√£o do modelo em um dado conjunto de dados de ‚Äúajuste" (*fit*). Associado ao ajuste, vamos ter o erro individual calculado para cada um dos pontos, e diversas outras medidas e formas de calcular a qualidade desse ajuste. A figura 1 mostra um exemplo de um ajuste linear no conjunto de dados de an√∫ncios.

![Figura 1 - Dataset de An√∫ncios](resources/01-anuncios.png)
*Figura 1 - Conjunto de dados de an√∫ncios. Da esquerda pra direita, temos as rela√ß√µes entre investimentos em TV x Vendas, investimentos em R√°dio x Vendas e investimentos em Jornais x Vendas*.

Como podemos ver, apesar do ajuste linear apresentar um bom ajuste ‚Äúvisual‚Äù nas rela√ß√µes de vendas com TV e R√°dio, vemos que ele apresenta um ajuste pior na rela√ß√£o entre vendas e investimentos em Jornais.

O ajuste linear √© um dos mais simples e menos complexos que podemos assumir entre dois conjuntos de dados. A verdade √© que, podemos aumentar a complexidade dos modelos que utilizamos para obter um ajuste melhor aos dados. A Figura 2 exemplifica isso melhor ao mostrar um ajuste n√£o linear na rela√ß√£o entre renda e anos de educa√ß√£o, apesar de uma rela√ß√£o linear tamb√©m poder ser obtida, por√©m com um erro maior.

![Figura 2 - Dataset de renda](resources/02-renda.png)
*Figura 2 - Conjunto de dados de renda. Na figura √† esquerda, os dados que relacionam Renda x Anos de Educa√ß√£o, na figura a esquerda, o polin√¥mio n√£o linear utilizado para gerar esses dados (adicionando erros randomicamente).*

Tamb√©m, na imagem √† direita da Figura 2 podemos perceber certas linhas em preto ligando os pontos at√© seu ajuste modelado. A dist√¢ncia de cada ponto vermelho at√© o ajuste em azul √© chamado de erro do modelo.

## Modelos param√©tricos *x* n√£o param√©tricos

O que percebemos nos exemplos retratados nas Figuras 1 e 2 √© que os modelos utilizados faziam alguma suposi√ß√£o inicial sobre o formato dos nossos dados. Isso √© uma caracter√≠stica de modelos chamados *param√©tricos*. Uma forma que eu gosto de pensar sobre √© que, justamente pela quest√£o de existir uma suposi√ß√£o sobre esse formato, o modelo apresenta um car√°ter enviesado (*bias/biased*) sobre os dados que est√° observando.

> üí° Vi√©s (ou tend√™ncia): ‚Äútend√™ncia geral ou determinada por for√ßas externas.‚Äù - *Oxford Languages*.

Al√©m deles, existem tamb√©m os modelos chamados *n√£o param√©tricos*. Esses, em quest√£o, n√£o fazem suposi√ß√µes acerca da forma dos dados e tentam encontrar o melhor ajuste poss√≠vel para cada um dos conjuntos apresentados, evitando assim modelos que n√£o descrevam bem o comportamento por assumir uma forma prematura. Esses tipos de modelos apresentam uma grande vantagem em rela√ß√£o aos param√©tricos por conseguirem se adequar a um espa√ßo maior de possibilidades de ajuste em diversos tipos de dados, introduzindo menos vi√©s no c√°lculo de erro total do modelo.

Geralmente, modelos n√£o param√©tricos costumam ser mais flex√≠veis e complexos que modelos param√©tricos, em troca de precisarem de mais dados. Um par√¢metro tamb√©m comum ao se abordar modelos n√£o param√©tricos mais complexos √© a quest√£o da interpretabilidade.

> üí° Interpretabilidade: descreve a possibilidade de compreender o modelo de aprendizado de m√°quina e de apresentar base para a tomada de decis√£o de uma maneira mais compreens√≠vel para humanos. - *Explainable Artificial Intelligence (XAI): Concepts, Taxonomies, Opportunities and Challenges toward Responsible AI, Alejandro Barredo Arrieta, et al.*

A verdade √© que, na maioria das vezes, quanto mais simples for o modelo, maior √© sua interpretabilidade. Pegue como exemplo, o ajuste linear feito na Figura 1 e perceba o qu√£o mais simples √© perceber e entender as rela√ß√µes feitas por esse ajuste. Modelos mais complexos perdem caracter√≠sticas de interpretabilidade, por performar diversas condi√ß√µes internas que n√£o s√£o facilmente entend√≠veis para n√≥s, humanos.  A Figura 3 mostra a rela√ß√£o de interpretabilidade x flexibilidade de alguns modelos existentes dentro do ecossistema de aprendizado de m√°quina.

![Figura 3 - Interpretability x Flexibility.](resources/03-flex-interp.png)
*Figura 3 - Rela√ß√£o interpretability x flexibility para alguns modelos conhecidos dentro do ecossistema de aprendizado de m√°quina (machine learning).*

Abordaremos ao longo desse artigo como a diferen√ßa entre esses tipos de modelos atua quando estamos avaliando da √≥tica da performance de modelo. Para seguir de agora em diante, utilizaremos um modelo de regress√£o para exemplificar as maneiras de medir a qualidade desse ajuste. Apesar de fugir ao escopo desse artigo, o pr√≥ximo par√°grafo faz uma pequena recapitula√ß√£o do que √© um modelo de regress√£o e da m√©trica que estaremos utilizando para medir a qualidade do ajuste.

Modelos de regress√£o s√£o modelos que tem como objetivo definir uma vari√°vel quantitativa como resposta (*target*). Uma aplica√ß√£o comum disso pode ser definir qual o sal√°rio m√©dio de uma pessoa baseado nos anos de estudo dela (como visto na Figura 2), ou o pre√ßo estimado de uma casa baseado em suas caracter√≠sticas (√°rea do lote, n√∫mero de quartos, proximidade de escolas, hospitais, etc). A medida mais comum no cen√°rio de regress√£o para endere√ßar a qualidade do ajuste √© o Erro M√©dio Quadr√°tico (*Mean-Squared Error*, ou *MSE*), onde temos a soma do quadrado de todas as dist√¢ncias entre o ponto predito e o verdadeiro (pense linha preta que liga a bolinha vermelha na linha azul na Figura 2), dividido pelo n√∫mero de amostras onde esse valor foi calculado (pros f√£s do ‚Äúmatematiqu√™s‚Äù, deixei a equa√ß√£o que representa o MSE abaixo).

$$
MSE = \frac{1}{2}\sum_{i=1}^{n}(y_i - \hat{f}(x_i))^{2}
$$

## Qualidade do ajuste e conjuntos de treino e teste

Quando queremos ensinar um comportamento para um modelo de aprendizado de m√°quina, √© comum querermos testar o qu√£o bom o nosso modelo foi em dados novos, mas n√£o s√≥ isso, dados novos que tenham r√≥tulos (*targets*) que nos permitam tamb√©m calcular a performance dele.

Para isso, uma estrat√©gia comum √© separar os dados em dois principais conjuntos, chamados de *treino* e *teste.* O modelo √© exposto aos dados de *treino* para aprender rela√ß√µes entre os dados e realizar o seu ajuste. Ap√≥s essa exposi√ß√£o, o modelo faz suas predi√ß√µes dos dados de *teste*. √â comum que o maior interesse seja em calcular as m√©tricas de qualidade de ajuste no conjunto de *teste*, que vai ditar o comportamento do modelo em dados que n√£o foram vistos previamente, mas isso n√£o nos impede de tirar informa√ß√µes importantes sobre a qualidade do nosso modelo tamb√©m aplicando essas m√©tricas no conjunto de *treino*.

Aqui, novamente, discutimos sobre o qu√£o flex√≠vel um modelo √©, e o quanto isso influencia nas medidas de qualidade de ajuste nos conjuntos de *treino* e de *teste*. Na Figura 4 √† esquerda, podemos ver um conjunto de dados gerados a partir da linha em preto com algum erro rand√¥mico adicionado, seguido de 3 modelos tentando realizar esse ajuste. J√° a figura a direita mostra o MSE de *treino* (em cinza) e de *teste* (em vermelho), com os pontos amarelo, azul e verde representando os modelos da figura a esquerda.

![Figura 4 - Modelo de exemplo 01](resources/04-model-example-01.png)
*Figura 4 - Na esquerda, a fun√ß√£o que gerou os pontos (em preto), seguido de 3 modelos ajustados: o amarelo (regress√£o linear), o azul (polin√¥mio) e o verde (algum modelo mais complexo de grau maior). Na direita, o MSE referente ao treino (em cinza) e ao teste (em vermelho) para cada um deles.*

O que n√≥s vemos √© que no *trade-off* entre flexibilidade do modelo e sua qualidade de ajuste, o modelo intermedi√°rio conseguiu o melhor resultado no teste, apesar do verde ter conseguido o melhor resultado no treino. A quest√£o √©: enquanto o modelo verde (mais flex√≠vel) foi capaz de aprender todas as caracter√≠sticas dos dados de *treino* apresentados pra ele, ele n√£o conseguiu generalizar bem em dados de *teste*. Chamamos essa caracter√≠stica onde o modelo aprende ‚Äúmuito bem‚Äù os dados de *treino* e n√£o consegue generalizar os dados de teste de *overfitting* (ou, no portugu√™s, sobreajuste).

Podemos reparar tamb√©m que o ajuste do modelo amarelo (menos flex√≠vel), gerou um valor de qualidade do ajuste alto tanto pra treino, quanto pra teste, mostrando que o modelo n√£o foi capaz de se ajustar bem aos dados de *treino*, nem generalizar nos dados de *teste.* Para esses modelos onde tanto a m√©trica de qualidade do ajuste √© alta para o *treino* e para o *teste*, e o modelo √© incapaz de generalizar qualquer coisa, damos o nome de *underfitting* (ou, no portugu√™s ajuste insuficiente). Discutiremos esses dois termos com mais calma logo a frente.

A Figura 5 mostra um outro exemplo semelhante a figura 4 onde podemos ver que a dist√¢ncia dos valores de qualidade dos ajustes de *treino* e *teste* se distanciam cada vez mais conforme a flexibilidade do modelo vai aumentando.

![Figura 5 - Modelo de exemplo 2](resources/05-model-example-02.png)
*Figura 5 - Na esquerda, a fun√ß√£o que gerou os pontos (em preto), seguido de 3 modelos ajustados: o amarelo (regress√£o linear), o azul (polin√¥mio) e o verde (algum modelo mais complexo de grau maior). Na direita, o MSE referente ao *treino* (em cinza) e ao *teste* (em vermelho) para cada um deles.*

O exemplo da Figura 5 se faz necess√°rio para mostrar um caso onde o modelo mais complexo ainda geraria um sobre ajuste, mas onde o modelo menos complexo n√£o necessariamente gerou um sob ajuste. J√° a Figura 6 mostra o oposto: um caso onde o modelo mais complexo e flex√≠vel teve uma boa performance, sendo muito superior ao modelo menos flex√≠vel. Para entender melhor porqu√™ essas situa√ß√µes acontecem, precisamos investigar melhor como o erro desses modelos √© calculado.

![Figura 6 - Modelo de exemplo 3](resources/06-model-example-03.png)
*Figura 6 - Na esquerda, a fun√ß√£o que gerou os pontos (em preto), seguido de 3 modelos ajustados: o amarelo (regress√£o linear), o azul (polin√¥mio) e o verde (algum modelo mais complexo de grau maior). Na direita, o MSE referente ao *treino* (em cinza) e ao *teste* (em vermelho) para cada um deles.*

## O c√°lculo do erro

Finalmente chegamos na se√ß√£o onde discutiremos mais amplamente como o erro do modelo √© calculado, e entender suas componentes. Essencialmente, todo erro √© composto por duas partes: uma parte referente ao modelo, e uma parte chamada de erro irredut√≠vel (ou *Bayes Error*). Esse erro irredut√≠vel, como o pr√≥prio nome diz, √© o erro independe da qualidade do modelo, representando uma certa quantidade de ru√≠do em nossos dados.

A outra se√ß√£o, referente ao modelo, tamb√©m √© dividida em duas partes: a vari√¢ncia (*variance*) e o quadrado do vi√©s (*squared bias*).

$$
Error = Variance + Bias^2 + Irreducible \ Error
$$

E, novamente, para os amantes do ‚Äúmatematiqu√™s‚Äù:

$$
E_{MSE}(y_0 - \hat{f}(x_0))^2 = \mathrm{Var}(\hat{f}(x_0)) + [\mathrm{Bias}(\hat{f}(x_0)))]^2 + \mathrm{Var}(\epsilon)
$$

J√° discutimos nesse artigo algumas no√ß√µes de vi√©s, normalmente gerado por fazer suposi√ß√µes sobre um formato espec√≠fico dos dados quando na verdade ele n√£o existe. A Figura 4 mostra isso bem comparando o modelo linear e o modelo polinomial. O vi√©s do modelo linear contribuiu tanto para que seu erro de *treino* quanto de *teste* fossem maiores, enquanto o vi√©s do modelo polinomial foi menor, apesar de ainda existir.

O que acontece na pr√°tica √© que conforme voc√™ diminui o vi√©s nos seus dados aumentando a flexibilidade do seu modelo, normalmente voc√™ est√° aumentando a vari√¢ncia dele. E agora √© hora de entendermos um pouco melhor esse termo.

> üí° Vari√¢ncia: ‚Äúa *Vari√¢ncia* se refere a quantidade de mudan√ßas que ter√≠amos em nossa fun√ß√£o de predi√ß√£o $\hat{f}$ se n√≥s a estim√°ssemos usando um conjunto de dados diferente.‚Äù

Olhando desse ponto de vista, se separ√°ssemos o nosso conjunto de *treino* em subconjuntos menores, por√©m significativos, e apresent√°ssemos cada um desses conjuntos para nosso modelo, a vari√¢ncia seria a quantidade necess√°ria de mudan√ßas com o intuito de reajustar o nosso modelo. Pensando no caso de uma simples reta, seu formato n√£o muda se apresentado a conjunto de dados diferentes, j√° no caso de um modelo n√£o param√©trico, seus ajustes fariam sua forma mudar com mais facilidade se os subconjuntos tivessem diferen√ßas expressivas entre si, aumentando a vari√¢ncia.

Fazendo uma pequena recapitula√ß√£o de tudo que n√≥s vimos at√© aqui:

- Sobre **erro**:
  - O erro √© composto por, essencialmente, duas partes: uma que pode ser reduzida, e uma irredut√≠vel (constante).
  - Essa parte redut√≠vel √© referente ao ajuste do modelo, e √© composta pelo vi√©s quadr√°tico e sua vari√¢ncia.
  - Erros existem em ambos os conjuntos: *treino* e *teste*, e saber analisar a rela√ß√£o entre eles √© necess√°rio:
    - Erro de *treino* alto e erro de *teste* alto: o modelo realizou um ajuste insuficiente nos dados de *treino (underfitting).*
    - Erro de *treino* baixo e erro de *teste* alto: o modelo realizou um sobreajuste nos dados de *treino* e se tornou incapaz de generalizar nos dados de *teste (overfitting)*.
    - Erro de *treino* baixo e erro de *teste* baixo: o modelo foi capaz de se ajustar de maneira satisfat√≥ria nos dados de *treino* e generalizar bem nos dados de *teste*.
- Sobre **vi√©s**:
  - O vi√©s √© adicionado ao assumir caracter√≠sticas sobre os dados que podem ou n√£o ser verdade, normalmente sendo seu formato, meio que como a gente pensaria na palavra ‚Äúvi√©s‚Äù no uso normal do portugu√™s: ‚Äúter um vi√©s sobre algo‚Äù.
  - Modelos param√©tricos fazem essas suposi√ß√µes sobre o formato dos dados, e portanto, possuem um vi√©s mais alto em casos onde essa suposi√ß√£o pode n√£o ser verdade. J√° modelos n√£o param√©tricos n√£o fazem essa suposi√ß√£o, e possuem vi√©s menor.
  - Geralmente, o vi√©s √© alto quando a flexibilidade do modelo √© baixa, e diminui conforme o modelo se torna mais flex√≠vel.
- Sobre **vari√¢ncia**:
  - A vari√¢ncia acontece conforme precisamos mudar nossa fun√ß√£o de representa√ß√£o do modelo conforme apresentamos novos dados para ele.
  - Modelos param√©tricos possuem um formato para a fun√ß√£o de representa√ß√£o e tentam ajust√°-la aos dados, possuindo baixa vari√¢ncia, j√° que a fun√ß√£o n√£o muda de formato ao ser apresentada a novos conjuntos de dados. Modelos n√£o param√©tricos, por n√£o possu√≠rem uma fun√ß√£o de representa√ß√£o pr√©-definida, se ajustam aos dados e podem acabar se ajustando a ru√≠do presente no conjunto, tendo assim uma vari√¢ncia maior.
  - Geralmente, a vari√¢ncia √© baixa quando a flexibilidade do modelo √© baixa, e aumenta conforme o modelo fica mais flex√≠vel.

Por fim, a Figura 7 condensa bem como cada um desses par√¢metros se comporta em rela√ß√£o a flexibilidade do modelo (linhas desenhadas de maneira arbitr√°ria pra representar o comportamento por alto).

![Figura 7 - Erros](resources/07-errors.png)
*Figura 7 - *Erros (Bias¬≤, Variance, Train, Test, Bayes)* x *Flexibilidade.*

## Considera√ß√µes finais

Medir a qualidade do ajuste de um modelo √© algo de grande import√¢ncia quando se est√° trabalhando com aprendizado de m√°quina. A verdade √© que n√£o existe um modelo perfeito ‚Äúbala de prata‚Äù que pode resolver todos os nossos problemas. Saber analisar o que as medidas de ajuste falam pra n√≥s √© importante para nos dar a capacidade de discernir quando utilizar cada modelo, sabendo os *trade-offs* que cada t√©cnica apresenta.

As vezes, √© ‚Äúok‚Äù adicionar um pouco de vi√©s e usar um modelo linear em uma situa√ß√£o que entender o que o modelo est√° fazendo √© mais importante em si do que sua predi√ß√£o. E as vezes, a predi√ß√£o importa mais do que tudo e a gente n√£o est√° olhando pra interpretabilidade do modelo, abrindo o leque para op√ß√µes mais complexas.

> üí° A verdade √© que por mais maravilhosas que as *Redes Neurais* sejam, as vezes uma linha reta resolve o seu problema.

Existe um meme que eu gosto muito que eu vou compartilhar aqui com o objetivo de fechar esse artigo de maneira mais leve, mas que retrata muito a realidade da maioria dos casos quando estamos pensando em como abordar um problema, pensando se um ferramental √© suficiente para tal tarefa:

![Figura 8 - Meme](resources/08-meme.png)

---

Agrade√ßo a todo mundo que leu at√© aqui! Desejo a todos um bom dia/tarde/noite e aceito coment√°rios sobre o pequeno artigo!

Um abra√ßo, **Gabriel**
